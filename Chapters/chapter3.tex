\chapter{Previous Knowledge}
\label{cha:previous_knowledge}

Before explaining the devloped path planner various given basics have to be covered. In the following chapter Genetic Algorithms and their various parts will explained in-depth, for implementations and details about which of these possible functions have been chosen for the developed software, see \ref{cha:algorithm_details}. The second part will explain the vehicle and it's limitations assumed for in the simulations as well as the representation used, based on \cite{12}. Afterwards the Simulation Software developed by the AG Echtzeitsysteme is introduced, part of this software is used in the program developed for this paper and will be explained in detail.

\section{About Genetic Algorithms}
\label{sec:previous_knowledge_ga}

A Genetic Algorithm (GA) is a search heuristic based on the "`survival of the fittest"' idea of Evolutionary Theory. It was developed by Holland in the 1970s \cite{15} and has since then proven itself in solving complex search problems. The idea is that the best possible solutions of an initial (usually randomly obtained) generation survive into the next generation, similar to natural selection where the strong individuals survive and get a higher chance of passing on their genes while weaker ones die out. This process is then repeated until an optimal solution is obtained. Any given GA consists of almost the same steps, however depending on which functions are implemented for these steps, and a number of general parameters, the resuls can drastically vary. This makes it easy to apply the GA to a large number of problems since the basic setup is always the same and it is easy to exchange only part of the algorithm for another one to try and get the best results. In the following the various steps of the GA are explained and several possible algorithms are given. Chapter \ref{cha:algorithm_details} explains which of these were used and why they were chosen over the other possibilities. Below is a pseudocode for a smaple GA.
\begin{verbatim}
  void function GA
    SET genCount to 0;
    initialization pop(genCount);
    evaluation pop(genCount);
    WHILE (genCount NOT EQUAL maxGenCount) do
      INCREMENT genCount;
      selection pop(genCount) from pop(genCount-1):
      crossover pop(genCount);
      mutation pop(genCount);
      evluation pop(genCount);
    ENDWHILE
\end{verbatim}

\subsection{Initialization}
\label{sec:initialization}

In order for the GA to do anything it first needs an initial generation which has to be obtained elsewhere. Unlike the following generations the initial population is usually generated randomly, but it may also be obtained from some previous computation done by another algorithm. Using a different algorithm to do preliminary computation, without letting it run so long that it would find an optimum solution, and then using the GA to optimize the solution can significantly outperform either of the two algorithms looking for an optimal solution in it's own. It can however also lead to a situation where the GA has no chance of finding the optimal solution and only finds a local otipimum since the initial generation it was given did not contain enough members of the best part of the graph.
How a random population can be obtained heavly depends on the representation of the given problem within the GA, the genome. The choice of genome is one of the most important when developing a GA and also one of the hardest since there are countless possible ways to encode a problem and no real way to tell how effective a representation may be without actually implementing it. In most cases a bit string representation is chosen where any bit represents a certain possible choice within the searchspace, for example whethere a certain vertex is part of the solution or not. For certain tasks other representations may be better but in most cases binary encoding is sufficient and makes the implementations of the other functions easier.

\subsection{Evaluation}
\label{sec:evaluation}

The second step in the algorithm is the evaluation, which, like the representation of the genome, is a function that is not standarized and is, as such, hard to choose and a big factor for the performance of the GA. The evaluation function assigns a fitness rating to every genome of the given population. Every genome describes a, not necessarily possible, solution to the given problem and the fitness rating describes how good a solution that is. The rating is higher for good solutions and lower bad ones, the exact scaling is not standarized and has to be adjusted to the given problem. For example, in some cases it might be best to remove impossible solutions from the population, in others it may be better to keep since they could still contain valuable parts for another solution. The scaling also depends on whether there is a way to know an optimal solution, in which case there would have to be a maximum fitness rating to assign to such, in other cases no maximum may be known.
The evaluation function depends on the problem to be solved and will usually require representations of the genome other than it's binary form. Functions to obtain one representaion from the other have to be implemented and some way to describe the usefulness of a solution has to be found.
Once the evaluation function is done, every genome in the currrent population should have a rating assigned to it, the consequences of this rating are defined by the other functions, mainly the selection.

\subsection{Selection}
\label{sec:selection}

The selection function takes $N$ individuals from the previous generation into the current one according to their fitness rating. The exact process of selecting and consideration of the rating depends on the algorithm employed. Since there are more algorithms, and even more variants of these algorithms, than can reasonably be explained here, we will concentrate on the possible functions considered or tested for our own GA. The simplest way to select $N$ individuals would be to simply take the $N$ best ones, this usually not done since we want to preserve a certain randomness for two reasons. One, individuals with a low fitness rating should still have a chance to be selected since they may still contain parts of a good solution which could be extracted from the bad part by either crossover or mutation. Second, we may want good solutions to be able to be selected several times to give them a higher chance to reproduce through crossover later. The most commonly known method which fulfills both these requierements is the fitness proportionate selection\cite{16}, also known as roulette-wheel selection. It simulated a biased roulette wheel by assigning each genome a segment of the wheel, proportional in size to the genome's fitness rating, and then spinning the wheel $N$ times. This algorithm takes the fitness rating into account while still giving bad solutions a chance to be selected, albeit a low one, and also allows for good individuals to be chosen several times.  If the scaling of the fitness function is bad however this could mean that either too many bad solutions survive, because their chance was not small enough, or it could starve out the algorithm because the gap between good and bad solutions is too big and the same good-but-not-optimal solution, also known as a super-individual, is selected almost every time. Stochastic Universal Sampling (SUS) is an optimized version the roulette-wheel algorithm which aims to remove bias, offer minimum spread and have a low computational complexity. SUS works by selecting a random value $r$ and then choosing individuals at evenly spaced intervals instead of generating a new random value every time. This gives weaker members a better chance to be selected instead of being dominated by the better solutions.\cite{17} A different approach to selection it the tournamend selection where several individuals are chosen at random and put in a tournament with the winner being selectd to the next generation. The trounament size is not fixed and can be adjusted for different needs, more competitors gives a weaker members a worse chance of making winning, a tournament size of $1$ would be the same as random selection. A tournament can either be won by the individual with the highest rating or one chosen at random with each member having a probability proportional to their rating.

\subsection{Crossover}
\label{sec:crossover}

The crossover function chooses two individuals from the population at random and recombines their elements in a certain way to produce two new individuals. One or both of these can be kept for the new population depending on the implemenentation. Crossover is considered to be the most important search operator\cite{16}. It's results depend on the crossover operator chosen, whether one or both children are kept and the crossover rate, that is the percentage of the new population that is aquired by crossover instead of selection. The original crossover operator proposed by Holland is the \textit{one-point crossover} which chooses one position in the genome at random and switches the bits to the right of this point bewteen the two chosen genomes. As such, one child ends up with the left part of parent one and the right part of parent two and the other child vice versa. This simple operator usually leads to inferior results however \cite{18,19,20}, so \textit{multi-point crossover} operators have been introduced. These work in the same manner as the \textit{one-point crossover} operator but choose $x$ points and switch $x$ times between values from the first and second parent. Depending on implementation the crossover points may also be fixed instead of randomly chosen. Empirical studies have shown an \textit{eight-point crossover} to be optimal \cite{21,22,19}, however the \textit{two-point crossover}  is the most common \cite{19}. The maximum number of crossover points is reached when the number points is equal to the number of bits in a genome, in which case we call it a \textit{uniform crossover}, which decices via fair coin toss for each bit whether it is taken from the first or second parent. This is usually done by generating a bit mask of the length of the genome instead of generating a new random boolean for each bit seperately. Other operators, which are not further explained here, include the \textit{segment crossover} \cite{19}, \textit{shuffle crossover} \cite{19} and \textit{punctuated crossover} \cite{23}.

\subsection{Mutation}
\label{sec:mutation}

The mutation operator is simple function that, with a small chance, randomly inverts a single bit of any given genome, regardless of whether this individual has been attained by selection or mutation. The purpose of this inversion is to re-introduce certain possibilities into the population that have died out and can not be gained by crossover anymore since no member of the population has the necessary combination of bits. The mutation rate is usually very low, 0.001, 0.01 or 0.005 are common \cite{20,23,24}.

\subsection{Reordering}
\label{sec:reordering}

Unlike the operators so far the reordering operator is optinal on not usually part of a GA. It works by randomly chooses an individual with a certain probability, for example 0.1, and then reverses the order of bits between two within this genome, these points are also chosen at random. In certain cases this inversion has been shown to significantly improve the performance of the GA by preventing what is called \textit{deception}\cite{8}. This can happen when certain bits in a genome are important but very far apart, called loose linkage. In such a case the crossover operator is likely to seperate these building blocks even though the need to be together, something that were less likely to happen if the blocks were closer together. Whether the reordering operator is appropriate depends on the given problem and also the crossover operator employed.

\subsection{Parameters}
\label{sec:parameters}

Once all the parts of the GA have been decided on there are only a number of parameters left to set. While there are some common practices around, most of these have to be decided by trying them out and comparing results. The most important parameters are population size, number of generations, crossover rate, mutation rate, generation gap and scaling. Population size simply determines how many individuals there are in any generation, too small a number will prevent the GA from covering the search spacen, too large a number will slow down computation. Population size can be anything from several hundred to several thousand and has to be tested out. Likewise, the number of generations determins how many times the algorithm runs and applies selection/crossover/mutation/evaluation, and has to be tried out. If an optimal solution is usually found after 40 or 50 iterations there is no need to continue further. This number is not necessarily set, it is a common way to determin the runtime of the algorithm, but alternative termination conditions can be set instead, for example for one individual to reach a certain fitness. The crossover rate determines the percentage of the new generation acquired by crossover instead of selection is usually set very high. 0.6, 0.75 and 0.95 are common values \cite{20, 22,24}. Just the same the mutation rate determines the chances of any given bit to be mutated, common values for have already been covered in \ref{sec:mutation}. The generation gap determines how many members of the generation are replaced in the next generation, which is usually set to 1.0, meaning the entire population is replaced. Scaling is applied to the fitness rating to prevent super-individuals from scewing the results of the selection early. The exact way of scaling depends on the evaluation function and may not be necessary at all, but a sigma-truncation is common. \cite{26}

\section{About the Vehicle}
\label{sec:previous_knowledge_vehicle}



\section{About the Simulation Software}
\label{sec:previous_knowledge_ezsystems}
