\chapter{Related Work}
\label{cha:related_work}

Path planning is a fundamental problem, so it is unsurprising there are already many possible solutions. Several well-known algorithms have been used to tackle this problem or combined in order to achieve better results. However, it is also a very complex problem and has been shown to be at least NP-complete and PSPACE-hard, with it's complexity growing exponentially with the configuration space, in our case the number of trailers. \cite{1} In the following chapter we will take a closer look at some of the solutions proposed and also consider their complexity and their problems with this particular task. This chapter is split in two parts, the first one concentrating on incremental algorithms and probabilistic metaheuristics, the second one instead focusing on other machine learning approaches. The focus in either case is the usefulness of the algorithm as a path-planning solution for a general-n-trailer. \pagebreak[4]

\section{Alternative Algorithms for Path-Planning}
\label{sec:alternative_pathplanning}

In order to plan a path using incremental algorithms we first have to convert our path planning into a graph searching problem. To this end we have to construct a suitable graph from a given configuration space, which in turn was constructed by representing any possible configuration, that is direction and position of all elements of the vehicle, as a single point in this C-space. There are several approaches to this task, for example the visibility graph approach \cite{2,3} and the retraction method \cite{4,5}, but for the purpose of this paper it is enough to know that such a conversion into a graph searching problem is possible. 

\subsection{A* Algorithm}
\label{sec:a_star}

The A* algorithm is one of the most widespread solutions for graph traversal problems.\cite{8} In order to utilize it for motion planning the area has to be overlayed with a configuration space and then transformed into a graph as described above. This leads to several problems for our path planning task. The complexity of the problem depends on the size of the graph, which in turn depends on the size of the configuration space. Two factors determine this size: The number of degrees of freedom our system has, in this case the number of trailers, as well as the magnitude of our grid, which depends on both the size of our map and the resolution of the overlayed grid. A finer grid leads to a larger number of vertics in the graph and, as the computation time of the A* algorithm grows exponentially with this number, results in slower performance. However, a coarse grid woud ignore possible paths since obstacles would appear greater than they are. To a certain point this can be useful to prevent collisions, but it also means a loss of information and consequently fewer options to choose from, possibly missing better paths. There may also be additionl constraints the path has to satisfy, which further slow down computation. So while A*, or similar graph searching algorithms, like Dijkstra's grapgh search \cite{6}, can be used, their performance for this kind of complex problem prevents them from being feasable.

\subsection{Hill Climbing}
\label{sec:hill_climbing}

Another common algorithm used to tackle graph searching problems is Hill Climbing. It is popular due to it's simplicity and ability to find a local optumum in a short amount of time, also it can return a result an any point, even when it is not yet finished. This may be important in real-time system, where it is more important to have a solution now than to have an optimized solution later. This may also hold true for motion planning task, when the speed of planning is more important than the quality of the resulting paths. In our give case however the quality of the path is more important than the speed of computation. In simple cases Hill Climbing produces paths of similar quality when compared to Simulated Annealing or Genetic Algorithms \cite{8}, however as the search space grows more complex the algorithm fails to generate good solutions. This is due to it's very local searching behaviour which becomes more of an obstacle the larger the search space gets. Various optimization techniques try to mitigate this problem, such as Stochastic Hill Climbing, which does not examine all neighbours but chooses randomly and then evaluates whether to move there or to examine another one, or Random-restart Hill Climbing, which tries to counter the locality issue by choosing it's start point at random and then repeat the entire seach several times. Even with these optimizations in place, Hill Climbing still cannot compete with other algorithms in solving problems of the complexity we consider here. Stochastic Hill Climbing produces paths similar to Standard Hill Climbing at equal or even greater cost \cite{8} and also fails to produce good results once the search space becomes too large. Random-restart Hill Climbing could potentially produce good results, but in a large search space this would very much rely on luck. Since the algorithm would have to run many times to give at least some chance that a globally good path has been found the performance would suffer greatly. Also, unlike A*, Hill Climbing can never guarantee that the optimal path has been found, this is also true for our Machine Learning Algorithms however.

\subsection{Simulated Annealing}
\label{sec:simulated_annealing}

Simulated Annealing is a probabilistic metaheuristic which searches for a optimum solution in a way similar to Hill Climbing, that is, by always considering the neighbours of the current position and then using a given function to determin whether or not to move to that neighbour. Unlike Hill Climbing however, Simulated Annealing changes it's behaviour over time, according to a global parameter $T$ (Temperature). $T$ can be defined freely, but always ends with $T=0$. The algorithm favours moves that go towards lower energy states the smaller $T$ is, so it starts out by ignoring local minima and moves loosely towards areas that contain good solutions overall. Then, with lower $T$, it starts prefering moves that go downhill the most so that it starts moving towards the local minima in the given region once $T$ gets close to $0$. Due to this Simulated Annealing circumvents the Hill Climbing problem of solely moving towards local solutions while ignoring the global ones. The algorithm shows results similar to those of a Genetic Algorithm but is significantly outperformed as far as number evaluation is concerned.\cite{8}

% Iterative Algorithms End

\section{Alternative Machine Learning Algorithms for Path-Planning}
\label{sec:alternative_machine_learning_pathplanning}

As we can see from the algorithms presented in \ref{alternative_pathplanning} an iterative approach causes a number of problems when it comes to complex tasks. Since we want to develop a solution for a general-n-trailer, with theoretically infinite degrees of freedom, an algorithm that stops producing feasible results for non-trivial problems is not an option. Of the three algorithms presented, Simulated Annealing is the only one that produces useful results, but also requires a very large number of computations \cite{8} for more complex problems. In order to avoid these issues a number of machine learning algorithms have been proposed to solve the motion planning problem. In the following section we will consider INSERT NUMBER HERE of these approaches, including a second implementation of the Genetic Algorithm (GA). This  GA is important since the comparison to Hill Climbing and Simulated Annealing refered to earlier have been made using that implementation and not the one developed in this paper. \cite{8}

\subsection{Reinforced Learning}
\label{sec:reinforced_learning}

Reinforced Learning is one category of Machine Learning algorithms with different capabilities depending on the method used. Before focusing further on any of these specific algorithms a general overview of the principles of Reinforced Learning will be given. The general principle of Reinforced Learning is to assign a certain reward to any action. This reward can be positive or negative (punishment) and it's value depends on the action taken. The task of the algorithm is to choose a path through this graph such that the accumulated reward at the end is maximal. Since always choosing the greater reward at any point obvisouly does not mean achieving the greates accumulated reward a balance between eploration and exploitation has to be found. Exploitation means picking the best choice available in the current state and exploration means picking sub optimal choices in the hopes that actions further down this part of the decision tree are overall better. For this an $\epsilon$-greedy algorithm with an $\epsilon$ value of $5\%$ is usually used, where a greater $\epsilon$ value means more exploration and a smaller value infers more exploitation. For path planning tasks a Markov process is usually assumed, which means that only current inputs, in our case the current position of the truck, are used and past ones are ignored. This means that old inputs do not have to be stored, so no memory is required for that, and computation becomes much simpler since fewer variables are considered. Reinforced Learning Algorithms can be split into three groups depending on their model of the environment and their bootstrapping capability. Bootstrapping means that estimates are updates based on other estimates, which speeds the algorithm up. Dynamic Programming methods (DP), Monte Carlo methods (MC) and Temporal Difference methods (TD). MC and TD don't need an exact model of the environment, where MC uses no bootstripping but TD does. DP also uses bootstrapping but needs an exact model of the environment, which makes it less feasable for motion-planning purposes.\cite{9} Specific Reinforced Learning implementations can be categorized according to these three methods. Both TD and MC methods can be split further depending on whether they are on-policy or off-policy, where on-policy is $\epsilon$-greedy and has a certain chance to take a random action rather than then currently best one, while off-policy is not random but suffers from a decreased learning speed as a consequence. In the following we will now have a closer look at INSERT NUMBER HERE such algorithms.

\subsubsection{Q-Learning}
\label{sec:q-learning}

Q-Learning is an off-policy, $\epsilon$-greedy TD method which learns an action-value function independent from the given policy to make an assumption about the optimal action-value function. Compared to other algorithms it is exploration heavy and will probably make more random decisions than explotive descision, due to this it is also more liekly to find the optimal policy in a finite number of episodes. Q-Learning requries the for actions and states to be defined. In our case states are sensor data, or our current positon, and actions are basic movement actions, here left/right in a certain angle and backwards driving. Since Reinforced Learning takes a large number of samples to learn from the first step should be done in a simulation, similar to the one developed for evaluation of our Genetic Algorithm. Simulations have the downside of being inaccurate since a model can never represent the real world perfectly, but can in turn be used much easier and much more quickly. Also data from a simulation can be stored and evaluated easily and the process can simply be automated. Once an optimal policy has been found using the simulation it has to be further refined using the actual vehicle. Depending on the accuracy of the simulation this will only require a small number of episodes. This phase may still be the most time consuming however since it cannot be sped up like the simulation. Depending on the vehicle and space required for the task, in our case a parking space and a truck, this task will still be the most challenging.

\subsubsection{Extended Q-Learning}
\label{sec:extended-q-learning}
Classical Q-Larning needs to make $m-1$ comparisons to maximazie the Q-value in a given state, so assuming n states the overall time-complexity of Q-Learning is $n(m-1)$. This can be optimized by using Extended Q-Leaning \cite{11}. We introduce an additional variable, $L_n$, for each state. $L_x$ is a lock bit and is 1 if the value $Q_x$ of the state $S_x$ is locked and won't be updated further, or 0 otherwise. At the beginning we have $L_n=0$ for all $S_n$ except the goal state. There are a number of properties that can be used to quickly set $L_n$ throughout the algorithm's runtime: 

\vspace{1\baselineskip}\vspace{-\parskip}

\begin{flushleft}
Property 1: If $L_n=1$ and $d_p_G > d_n_G$ then $Q_p = \gamma \times Q_n$ and set $L_p=1$\\
Property 2: If $L_n=0$ and $d_p_G > d_n_G$ then $Q_p = Max(Q_p,\gamma \times Q_n)$\\
Property 3: If $L_n=1$ and $d_n_G > d_p_G$ then $Q_n = \gamma \times Q_p$ and set $L_n=1$\\
Property 4: If $L_n=0$ and $d_n_G > d_p_G$ then $Q_n = Max(Q_n,\gamma \times Q_p)$\\
\end{flushleft}

\vspace{1\baselineskip}\vspace{-\parskip}

Using these properties both $L_i$ and $Q_i$ can be updated accordingly. Now, in order to determine if a state is optimal, we only need to check whether or not it is locked. So for $n$ states we only need $n$ comparisons, so we save $n(m-1)-n=nm-2n=n(m-2)$ \cite{11} comparisons.
For space complexity we previously had $n \times m $ for classical Q-Learning whereas we only need $3$ values for any state in the Extended Q-Learning Algorithm: Q-Value, $L_x$ and the best action at that state. This means a space-complexity of only $3 \times n$ and a saving of $nm-3n=n(m-3)$.\cite{11}


\subsubsection{R-Learning}
\label{sec:r-learning}

\subsubsection{Sarsa}
\label{sec:sarsa}

\subsubsection{Actor-critic}
\label{sec:actor-critic}

\subsection{Genetic Algorithm}
\label{sec:genetic_algorithm}

